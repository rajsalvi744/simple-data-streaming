# Real-Time Data Streaming System  

A robust real-time data streaming pipeline built to process and store large volumes of data with minimal delay. This end-to-end solution integrates powerful technologies to handle high-throughput streaming, on-the-fly processing, and scalable storage, making real-time data instantly accessible for analysis and applications.  

## üõ†Ô∏è Technologies Used  
- **Python**: Core programming language for orchestrating and integrating components.  
- **Kafka**: For streaming high-throughput data in real-time.  
- **Apache Spark**: To process and analyze data on the fly.  
- **Cassandra**: For fast and scalable storage of processed data.  
- **Docker**: To containerize all components for seamless deployment and scalability.  

## üìö Project Overview  
This project simulates a real-world use case where high-velocity data generated by an API needs to be processed and stored with minimal latency. Here's how it works:  
1. **Data Ingestion**: Kafka streams real-time data from an external API.  
2. **Data Processing**: Spark processes the data as it streams, performing transformations and computations on the fly.  
3. **Data Storage**: The processed data is streamed to Cassandra, a NoSQL database optimized for high availability and low latency.  

By combining these technologies, the system ensures real-time insights and provides a solid foundation for applications that demand quick data turnaround.  

## üöÄ Features  
- **Real-Time Data Ingestion**:  
  - Streams high-throughput data from an API using Kafka.  

- **On-the-Fly Data Processing**:  
  - Processes data in real-time with Apache Spark, enabling instant transformations and computations.  

- **Scalable Data Storage**:  
  - Stores processed data in Cassandra, designed to handle massive data volumes with low-latency access.  

- **Containerized Architecture**:  
  - Every component is containerized with Docker, ensuring easy deployment, portability, and scalability.  

## üåü Why This Matters  
Real-time data processing is essential for applications like monitoring, fraud detection, predictive analytics, and more. This project demonstrates how to build a scalable pipeline to meet these needs, using open-source tools and a modular approach.  

## üì∑ Screenshots or Architecture Diagram  
*(Add diagrams or visuals of the pipeline architecture or the tools in action here.)*  

## üîß How to Run  
1. **Clone the Repository**:  
   ```bash  
   git clone https://github.com/rajsalvi744/simple-data-streaming.git
   cd simple-data-streaming  
2. Set Up Docker:
Ensure Docker is installed and start the services using:
   ```bash
   docker-compose up  
3. Configure Kafka Topics:
Use the Kafka CLI or dashboard to set up topics for streaming data.
4. Stream Data:
Simulate API data generation using the provided Python scripts or your custom data source.
5. Query Cassandra:
Access the Cassandra database to verify and analyze the processed data.
6. Monitor Spark Jobs:
Use the Spark UI to monitor real-time processing jobs.
